{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tune a model"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/restructuredtext",
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    ":doc:`Previously <./train-a-model>` we crudely trained a simple :class:`~graph_pes.models.PaiNN` model on a subset of the `QM7 dataset <https://jla-gardner.github.io/load-atoms/datasets/QM7.html>`__.\n",
    "\n",
    "Let's now take this model (``./graph-pes-results/quickstart-run/model.pt``) and fine-tune it on the `C-GAP-17 dataset <https://jla-gardner.github.io/load-atoms/datasets/C-GAP-17.html>`__."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/restructuredtext",
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "We use some slightly different settings to last time:\n",
    "\n",
    "* the energy labels in the C-GAP-17 dataset are offset by [TODO]. We therefore use a :class:`~graph_pes.models.AdditionModel` composed of our existing model and a :class:`~graph_pes.models.FixedOffset` to account for this offset.\n",
    "* we use the :func:`~graph_pes.models.load_model` helper function to load our existing model weights\n",
    "* the C-GAP-17 has both energy and force labels, and so we train on both of these quantities\n",
    "\n",
    ".. literalinclude:: fine-tune-config.yaml\n",
    "    :language: yaml\n",
    "    :caption: fine-tune-config.yaml\n",
    "    :linenos:"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
